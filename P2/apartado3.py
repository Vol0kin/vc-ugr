# -*- coding: utf-8 -*-
"""p2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X0F592OFYA6PTl0BqwMzlXtSyfoBwUQ9
"""

# -*- coding: utf-8 -*-
"""EsquemaParte3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ge_Pw-txgQ1PRJTdJPioBOl02yILeVuf
"""

#########################################################################
################### OBTENER LA BASE DE DATOS ############################
#########################################################################

# Descargar las imágenes de http://www.vision.caltech.edu/visipedia/CUB-200.html
# Descomprimir el fichero.
# Descargar también el fichero list.tar.gz, descomprimirlo y guardar los ficheros
# test.txt y train.txt dentro de la carpeta de imágenes anterior. Estos 
# dos ficheros contienen la partición en train y test del conjunto de datos.

##### EN CASO DE USAR COLABORATORY
# Sube tanto las imágenes como los ficheros text.txt y train.txt a tu drive.
# Después, ejecuta esta celda y sigue las instrucciones para montar 
# tu drive en colaboratory.

# Descomentar estas lineas
#from google.colab import drive
#drive.mount('/content/drive')

#########################################################################
################ CARGAR LAS LIBRERÍAS NECESARIAS ########################
#########################################################################

# Terminar de rellenar este bloque con lo que vaya haciendo falta

# Importar librerías necesarias
import numpy as np
import keras
import keras.utils as np_utils
from keras.preprocessing.image import load_img, img_to_array
import matplotlib.pyplot as plt

# Importar el optimizador a usar
from keras.optimizers import SGD

# Importar modelos y capas específicas que se van a usar
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Model, Sequential
from keras.layers import Dense, Dropout


# Importar el modelo ResNet50 y su respectiva función de preprocesamiento,
# que es necesario pasarle a las imágenes para usar este modelo
from keras.applications.resnet50 import ResNet50, preprocess_input


# Importar el optimizador a usar
from keras.optimizers import SGD

#########################################################################
################## FUNCIÓN PARA LEER LAS IMÁGENES #######################
#########################################################################

# Dado un fichero train.txt o test.txt y el path donde se encuentran los
# ficheros y las imágenes, esta función lee las imágenes
# especificadas en ese fichero y devuelve las imágenes en un vector y 
# sus clases en otro.

def leerImagenes(vec_imagenes, path):
    clases = np.array([img.split('/')[0] for img in vec_imagenes])
    imagenes = np.array([img_to_array(load_img(path + "/" + img, 
                                                target_size = (224, 224))) 
                        for img in vec_imagenes])
    return imagenes, clases

#########################################################################
############# FUNCIÓN PARA CARGAR EL CONJUNTO DE DATOS ##################
#########################################################################

# Usando la función anterior, y dado el path donde se encuentran las
# imágenes y los archivos "train.txt" y "test.txt", devuelve las 
# imágenes y las clases de train y test para usarlas con keras
# directamente.

def cargarDatos(path):
    # Cargamos los ficheros
    train_images = np.loadtxt(path + "/train.txt", dtype = str)
    test_images = np.loadtxt(path + "/test.txt", dtype = str)
    
    # Leemos las imágenes con la función anterior
    train, train_clases = leerImagenes(train_images, path)
    test, test_clases = leerImagenes(test_images, path)
    
    # Pasamos los vectores de las clases a matrices 
    # Para ello, primero pasamos las clases a números enteros
    clases_posibles = np.unique(np.copy(train_clases))
    for i in range(len(clases_posibles)):
        train_clases[train_clases == clases_posibles[i]] = i
        test_clases[test_clases == clases_posibles[i]] = i

    # Después, usamos la función to_categorical()
    train_clases = np_utils.to_categorical(train_clases, 200)
    test_clases = np_utils.to_categorical(test_clases, 200)
    
    # Barajar los datos
    train_perm = np.random.permutation(len(train))
    train = train[train_perm]
    train_clases = train_clases[train_perm]

    test_perm = np.random.permutation(len(test))
    test = test[test_perm]
    test_clases = test_clases[test_perm]
    
    return train, train_clases, test, test_clases

#########################################################################
######## FUNCIÓN PARA OBTENER EL ACCURACY DEL CONJUNTO DE TEST ##########
#########################################################################

# Esta función devuelve el accuracy de un modelo, definido como el 
# porcentaje de etiquetas bien predichas frente al total de etiquetas.
# Como parámetros es necesario pasarle el vector de etiquetas verdaderas
# y el vector de etiquetas predichas, en el formato de keras (matrices
# donde cada etiqueta ocupa una fila, con un 1 en la posición de la clase
# a la que pertenece y 0 en las demás).

def calcularAccuracy(labels, preds):
    labels = np.argmax(labels, axis = 1)
    preds = np.argmax(preds, axis = 1)
    
    accuracy = sum(labels == preds)/len(labels)
    
    return accuracy

#########################################################################
## FUNCIÓN PARA PINTAR LA PÉRDIDA Y EL ACCURACY EN TRAIN Y VALIDACIÓN ###
#########################################################################

# Esta función pinta dos gráficas, una con la evolución de la función
# de pérdida en el conjunto de train y en el de validación, y otra
# con la evolución del accuracy en el conjunto de train y en el de
# validación. Es necesario pasarle como parámetro el historial
# del entrenamiento del modelo (lo que devuelven las funciones
# fit() y fit_generator()).

def mostrarEvolucion(hist):

    loss = hist.history['loss']
    val_loss = hist.history['val_loss']
    plt.plot(loss)
    plt.plot(val_loss)
    plt.legend(['Training loss', 'Validation loss'])
    plt.show()

    acc = hist.history['acc']
    val_acc = hist.history['val_acc']
    plt.plot(acc)
    plt.plot(val_acc)
    plt.legend(['Training accuracy', 'Validation accuracy'])
    plt.show()

# Descomentar y modificar esta linea si se trabaja en colab
#!unzip /content/drive/My\ Drive/imagenes.zip -d /content/

###############################################################################
# 1. Extraccion de caracteristicas

# Cargar los datos
x_train, y_train, x_test, y_test = cargarDatos('imagenes')

# Definir parametros de entrenamiento
batch_size = 32
epochs = 35
optimizer = SGD()

# ImageDataGenerator usado para preprocesar la entrada de Resnet50
datagen_resnet50 = ImageDataGenerator(
    preprocessing_function=preprocess_input
)

# Definir el modelo ResNet50 (preentrenado en ImageNet y sin la última capa).
resnet50 = ResNet50(include_top=False, weights='imagenet', pooling='avg')

# Obtener características de entrenamiento y test utilizando Resnet50
features_train = resnet50.predict_generator(
    datagen_resnet50.flow(x_train, batch_size=1, shuffle=False),
    steps=len(x_train),
    verbose=1
)

features_test = resnet50.predict_generator(
    datagen_resnet50.flow(x_test, batch_size=1, shuffle=False),
    steps=len(x_test),
    verbose=1
)

# Crear modelo base 256x200 neuronas FC
base_model = Sequential()
base_model.add(Dense(256, activation='relu', input_shape=(2048,)))
base_model.add(Dense(200, activation='softmax'))

# Compilar el modelo
base_model.compile(
    loss=keras.losses.categorical_crossentropy,
    optimizer=optimizer,
    metrics=['accuracy']
)

# Mostrar modelo
print(base_model.summary())

# Obtener pesos del modelo base
base_weights = base_model.get_weights()

# Entrenar modelo
history = base_model.fit(
    features_train,
    y_train,
    validation_split=0.1,
    epochs=epochs,
    batch_size=batch_size,
    verbose=1
)

# Mostrar graficas de evolucion
mostrarEvolucion(history)

# Predecir los datos
prediction = base_model.predict(
    features_test,
    batch_size=batch_size,
    verbose=1
)


# Obtener accuracy de test y mostrarla
accuracy = calcularAccuracy(y_test, prediction)
print('Test accuracy: {}'.format(accuracy))

# Bajar epocas a 25
epochs = 25

# Crear modelo base 512x200 neuronas FC
base_model2 = Sequential()
base_model2.add(Dense(512, activation='relu', input_shape=(2048,)))
base_model2.add(Dense(200, activation='softmax'))

# Compilar el modelo
base_model2.compile(
    loss=keras.losses.categorical_crossentropy,
    optimizer=optimizer,
    metrics=['accuracy']
)

# Mostrar modelo
print(base_model2.summary())

# Obtener pesos del modelo base
base2_weights = base_model2.get_weights()

# Entrenar modelo
history = base_model2.fit(
    features_train,
    y_train,
    validation_split=0.1,
    epochs=epochs,
    batch_size=batch_size,
    verbose=1
)

# Mostrar graficas de evolucion
mostrarEvolucion(history)

# Crear modelo base 1024x200 neuronas FC
base_model3 = Sequential()
base_model3.add(Dense(1024, activation='relu', input_shape=(2048,)))
base_model3.add(Dense(200, activation='softmax'))

# Compilar el modelo
base_model3.compile(
    loss=keras.losses.categorical_crossentropy,
    optimizer=optimizer,
    metrics=['accuracy']
)

# Mostrar modelo
print(base_model3.summary())

# Obtener pesos del modelo base
base3_weights = base_model3.get_weights()

# Entrenar modelo
history = base_model3.fit(
    features_train,
    y_train,
    validation_split=0.1,
    epochs=epochs,
    batch_size=batch_size,
    verbose=1
)

# Mostrar graficas de evolucion
mostrarEvolucion(history)

# Crear modelo base 256x200 neuronas FC regularizado
model_reg1 = Sequential()
model_reg1.add(Dropout(0.2, input_shape=(2048,)))
model_reg1.add(Dense(256, activation='relu'))
model_reg1.add(Dense(200, activation='softmax'))

# Compilar el modelo
model_reg1.compile(
    loss=keras.losses.categorical_crossentropy,
    optimizer=optimizer,
    metrics=['accuracy']
)

# Mostrar modelo
print(model_reg1.summary())

# Obtener pesos del modelo base
reg1_weights = model_reg1.get_weights()

# Entrenar modelo
history = model_reg1.fit(
    features_train,
    y_train,
    validation_split=0.1,
    epochs=epochs,
    batch_size=batch_size,
    verbose=1
)

# Mostrar graficas de evolucion
mostrarEvolucion(history)

# Crear modelo base 512x200 neuronas FC regularizado
model_reg2 = Sequential()
model_reg2.add(Dropout(0.2, input_shape=(2048,)))
model_reg2.add(Dense(512, activation='relu'))
model_reg2.add(Dense(200, activation='softmax'))

# Compilar el modelo
model_reg2.compile(
    loss=keras.losses.categorical_crossentropy,
    optimizer=optimizer,
    metrics=['accuracy']
)

# Mostrar modelo
print(model_reg2.summary())

# Obtener pesos del modelo base
reg2_weights = model_reg2.get_weights()

# Entrenar modelo
history = model_reg2.fit(
    features_train,
    y_train,
    validation_split=0.1,
    epochs=epochs,
    batch_size=batch_size,
    verbose=1
)

# Mostrar graficas de evolucion
mostrarEvolucion(history)

# Crear modelo base 1024x200 neuronas FC regularizado
model_reg3 = Sequential()
model_reg3.add(Dropout(0.2, input_shape=(2048,)))
model_reg3.add(Dense(1024, activation='relu'))
model_reg3.add(Dense(200, activation='softmax'))

# Compilar el modelo
model_reg3.compile(
    loss=keras.losses.categorical_crossentropy,
    optimizer=optimizer,
    metrics=['accuracy']
)

# Mostrar modelo
print(model_reg3.summary())

# Obtener pesos del modelo base
reg3_weights = model_reg3.get_weights()

# Entrenar modelo
history = model_reg3.fit(
    features_train,
    y_train,
    validation_split=0.1,
    epochs=epochs,
    batch_size=batch_size,
    verbose=1
)

# Mostrar graficas de evolucion
mostrarEvolucion(history)

# Predecir los datos
prediction = model_reg3.predict(
    features_test,
    batch_size=batch_size,
    verbose=1
)

# Obtener accuracy de test y mostrarla
accuracy = calcularAccuracy(y_test, prediction)
print('Test accuracy: {}'.format(accuracy))

###############################################################################
# 2. Fine-tuning

# Definir el modelo ResNet50 (preentrenado en ImageNet y sin la última capa).
resnet50 = ResNet50(include_top=False, weights='imagenet', pooling='avg')

# Definir parametros de entrenamiento
batch_size = 32
epochs = 10
optimizer = SGD()

# Definir un objeto de la clase ImageDataGenerator para train y otro para test
# con sus respectivos argumentos.
datagen_train = ImageDataGenerator(
    validation_split=0.1,
    preprocessing_function=preprocess_input
)

datagen_test = ImageDataGenerator(
    preprocessing_function=preprocess_input
)

train_iter = datagen_train.flow(
    x_train,
    y_train,
    batch_size=batch_size,
    subset='training'
)

validation_iter = datagen_train.flow(
    x_train,
    y_train,
    batch_size=batch_size,
    subset='validation'
)

# Crear Resnet50 + FC con Dropout
x = resnet50.output
x = Dropout(0.2)(x)
x = Dense(256, activation='relu')(x)
x = Dense(200, activation='softmax')(x)

model1 = Model(inputs=resnet50.input, outputs=x)

# Compilar el modelo
model1.compile(
    loss=keras.losses.categorical_crossentropy,
    optimizer=optimizer,
    metrics=['accuracy']
)

# Guardar pesos del modelo
weights1 = model1.get_weights()

# Entrenar modelo
history = model1.fit_generator(
    train_iter,
    steps_per_epoch=len(x_train)*0.9/batch_size,
    epochs=epochs,
    validation_data=validation_iter,
    validation_steps=len(x_train)*0.1/batch_size
)

# Mostrar graficas de evolucion
mostrarEvolucion(history)

# Predecir los datos
prediction = model1.predict_generator(
    datagen_test.flow(x_test, batch_size=1, shuffle=False),
    steps=len(x_test),
    verbose=1
)

# Obtener accuracy de test y mostrarla
accuracy = calcularAccuracy(y_test, prediction)
print('Test accuracy: {}'.format(accuracy))

# Crear Resnet50 + FC con Dropout
x = resnet50.output
x = Dropout(0.2)(x)
x = Dense(512, activation='relu')(x)
x = Dense(200, activation='softmax')(x)

model2 = Model(inputs=resnet50.input, outputs=x)

# Compilar el modelo
model2.compile(
    loss=keras.losses.categorical_crossentropy,
    optimizer=optimizer,
    metrics=['accuracy']
)

# Guardar pesos del modelo
weights2 = model2.get_weights()

# Entrenar modelo
history = model2.fit_generator(
    train_iter,
    steps_per_epoch=len(x_train)*0.9/batch_size,
    epochs=epochs,
    validation_data=validation_iter,
    validation_steps=len(x_train)*0.1/batch_size
)

# Mostrar graficas de evolucion
mostrarEvolucion(history)

# Crear Resnet50 + FC con Dropout
x = resnet50.output
x = Dropout(0.2)(x)
x = Dense(1024, activation='relu')(x)
x = Dense(200, activation='softmax')(x)

model3 = Model(inputs=resnet50.input, outputs=x)

# Compilar el modelo
model3.compile(
    loss=keras.losses.categorical_crossentropy,
    optimizer=optimizer,
    metrics=['accuracy']
)

# Guardar pesos del modelo
weights3 = model3.get_weights()

# Entrenar modelo
history = model3.fit_generator(
    train_iter,
    steps_per_epoch=len(x_train)*0.9/batch_size,
    epochs=epochs,
    validation_data=validation_iter,
    validation_steps=len(x_train)*0.1/batch_size
)

# Mostrar graficas de evolucion
mostrarEvolucion(history)

####################################################################
# Reentrenar red con aumento de datos

# Definir objeto de ImageDataGenerator para aumento de datos con flip horizontal
datagen_aug = ImageDataGenerator(
    validation_split=0.1,
    preprocessing_function=preprocess_input,
    horizontal_flip=True
)

train_aug = datagen_train.flow(
    x_train,
    y_train,
    batch_size=batch_size,
    subset='training'
)

validation_aug = datagen_train.flow(
    x_train,
    y_train,
    batch_size=batch_size,
    subset='validation'
)

# Definir objeto de ImageDataGenerator para aumento de datos
datagen_fr = ImageDataGenerator(
    validation_split=0.1,
    preprocessing_function=preprocess_input,
    horizontal_flip=True,
    rotation_range=90
)

train_fr = datagen_train.flow(
    x_train,
    y_train,
    batch_size=batch_size,
    subset='training'
)

validation_fr = datagen_train.flow(
    x_train,
    y_train,
    batch_size=batch_size,
    subset='validation'
)

# Reestablecer pesos
model3.set_weights(weights3)

# Entrenar modelo
history = model3.fit_generator(
    train_fr,
    steps_per_epoch=len(x_train)*0.9/batch_size,
    epochs=epochs,
    validation_data=validation_fr,
    validation_steps=len(x_train)*0.1/batch_size
)

# Mostrar graficas de evolucion
mostrarEvolucion(history)

# Reestablecer pesos
model3.set_weights(weights3)

# Entrenar modelo
history = model3.fit_generator(
    train_aug,
    steps_per_epoch=len(x_train)*0.9/batch_size,
    epochs=epochs,
    validation_data=validation_aug,
    validation_steps=len(x_train)*0.1/batch_size
)

# Mostrar graficas de evolucion
mostrarEvolucion(history)

# Predecir los datos
prediction = model3.predict_generator(
    datagen_test.flow(x_test, batch_size=1, shuffle=False),
    steps=len(x_test),
    verbose=1
)

# Obtener accuracy de test y mostrarla
accuracy = calcularAccuracy(y_test, prediction)
print('Test accuracy: {}'.format(accuracy))
